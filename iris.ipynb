{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4874e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "936af667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris=datasets.load_iris()\n",
    "X=iris.data\n",
    "y=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b4099008",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y))\n",
    "y = np.eye(num_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3ed4b746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (120, 4)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"x_train shape:\", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a7c7f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train= np.array(x_train)\n",
    "y_train= np.array(y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6b9880a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test= np.array(y_val)\n",
    "x_test= np.array(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3845f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = x_train.shape[1]\n",
    "hidden1=32\n",
    "hidden2=16\n",
    "output_size=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "841b51ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ea7c3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input-->hidden1-->hidden2-->output\n",
    "W1=np.random.randn(input_size, hidden1)*0.01\n",
    "b1=np.zeros((1, hidden1))\n",
    "\n",
    "W2=np.random.randn(hidden1, hidden2)*0.01\n",
    "b2=np.zeros((1, hidden2))\n",
    "\n",
    "W3=np.random.randn(hidden2, output_size)*0.01\n",
    "b3=np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e3d25",
   "metadata": {},
   "source": [
    "relu\n",
    "relu_derivative\n",
    "softmax\n",
    "compute_loss_and_acc\n",
    "forward\n",
    "backward\n",
    "update_params\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fe38e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU Activation\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b8588d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(Z):\n",
    "    return (Z > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6f4c4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "697f69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_and_accuracy(y_true, y_pred):\n",
    "    eps = 1e-12\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)  # prevent log(0)\n",
    "    m = y_true.shape[0]\n",
    "    log_likelihood = -np.sum(y_true * np.log(y_pred), axis=1)\n",
    "    loss = np.mean(log_likelihood)\n",
    "    accuracy = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1))\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e561235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X):\n",
    "    # X: (m, input_size)\n",
    "    Z1 = X.dot(W1) + b1            # (m, hidden_size)\n",
    "    A1 = relu(Z1)\n",
    "\n",
    "    Z2 = A1.dot(W2) + b2           # (m, hidden_size)\n",
    "    A2 = relu(Z2)\n",
    "\n",
    "    Z3 = A2.dot(W3) + b3           # (m, output_size)\n",
    "    A3 = softmax(Z3)\n",
    "\n",
    "    cache = (X, Z1, A1, Z2, A2, Z3, A3)\n",
    "    return A3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f1a90ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(cache, Y):\n",
    "    X, Z1, A1, Z2, A2, Z3, A3 = cache\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # dL/dZ3\n",
    "    dZ3 = (A3 - Y) / m                      # (m, output_size)\n",
    "    dW3 = A2.T.dot(dZ3)                     # (hidden_size, output_size)\n",
    "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
    "\n",
    "    dA2 = dZ3.dot(W3.T)                     # (m, hidden_size)\n",
    "    dZ2 = dA2 * relu_derivative(Z2)         # (m, hidden_size)\n",
    "    dW2 = A1.T.dot(dZ2)                     # (hidden_size, hidden_size)\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "    dA1 = dZ2.dot(W2.T)                     # (m, hidden_size)\n",
    "    dZ1 = dA1 * relu_derivative(Z1)         # (m, hidden_size)\n",
    "    dW1 = X.T.dot(dZ1)                      # (input_size, hidden_size)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "    grads = (dW1, db1, dW2, db2, dW3, db3)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5264c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgrade_parameters(grads, learning_rate):\n",
    "    global W1, b1, W2, b2, W3, b3\n",
    "    dW1, db1, dW2, db2, dW3, db3 = grads\n",
    "    \n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4ff4653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    A3, _ = forward_propagation(X)\n",
    "    return np.argmax(A3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2b85fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 32\n",
    "epochs = 50\n",
    "lr = 0.05\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "17a90001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/50  Loss: 1.0920  Acc: 0.3417\n",
      "Epoch   5/50  Loss: 1.0887  Acc: 0.3417\n",
      "Epoch  10/50  Loss: 1.0808  Acc: 0.3583\n",
      "Epoch  15/50  Loss: 1.0631  Acc: 0.8083\n",
      "Epoch  20/50  Loss: 1.0155  Acc: 0.6750\n",
      "Epoch  25/50  Loss: 0.9052  Acc: 0.6583\n",
      "Epoch  30/50  Loss: 0.7642  Acc: 0.6583\n",
      "Epoch  35/50  Loss: 0.6223  Acc: 0.6917\n",
      "Epoch  40/50  Loss: 0.5024  Acc: 0.7833\n",
      "Epoch  45/50  Loss: 0.4391  Acc: 0.8750\n",
      "Epoch  50/50  Loss: 0.3664  Acc: 0.9750\n"
     ]
    }
   ],
   "source": [
    "n_samples = x_train.shape[0]\n",
    "num_batches = int(np.ceil(n_samples / batch_size))\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # shuffle\n",
    "    perm = np.random.permutation(n_samples)\n",
    "    X_shuf = x_train[perm]\n",
    "    Y_shuf = y_train[perm]\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        X_batch = X_shuf[start:end]\n",
    "        Y_batch = Y_shuf[start:end]\n",
    "\n",
    "        # forward\n",
    "        Y_hat, cache = forward(X_batch)\n",
    "\n",
    "        # loss & acc\n",
    "        loss, acc = compute_loss_and_accuracy(Y_batch, Y_hat)\n",
    "        epoch_loss += loss * X_batch.shape[0]\n",
    "        epoch_acc += acc * X_batch.shape[0]\n",
    "\n",
    "        # backward\n",
    "        grads = backward(cache,Y_batch)\n",
    "\n",
    "        # update\n",
    "        upgrade_parameters(grads, lr)\n",
    "\n",
    "    epoch_loss /= n_samples\n",
    "    epoch_acc /= n_samples\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:3d}/{epochs}  Loss: {epoch_loss:.4f}  Acc: {epoch_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8bc7cfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished. Final loss: 7.7337, final accuracy: 0.9167\n"
     ]
    }
   ],
   "source": [
    "Y_hat_all, _ = forward(x_train)\n",
    "final_loss, final_acc = compute_loss_and_accuracy(Y_hat_all, y_train)\n",
    "print(f\"\\nTraining finished. Final loss: {final_loss:.4f}, final accuracy: {final_acc:.4f}\")\n",
    "\n",
    "def predict(X):\n",
    "    Y_hat, _ = forward(X)\n",
    "    return np.argmax(Y_hat, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
